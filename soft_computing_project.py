# -*- coding: utf-8 -*-
"""Soft Computing Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SKjU26FGmlIkipUVyZKOapSIhVY8Lkne
"""

!pip install pyswarm

!pip install deap xgboost scikit-learn

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from xgboost import XGBClassifier
from deap import base, creator, tools, algorithms
import random
import matplotlib.pyplot as plt
import seaborn as sns

import kagglehub

# Download latest version
path = kagglehub.dataset_download("uciml/human-activity-recognition-with-smartphones")

print("Path to dataset files:", path)

import os
os.listdir(path)

import pandas as pd

train=pd.read_csv(path+'/train.csv')
test=pd.read_csv(path+'/test.csv')

train.shape

test.shape

train.dtypes

train.columns

test.columns

"""Preprocessing"""

# Drop 'subject' column
train = train.drop(columns=['subject'])
test = test.drop(columns=['subject'])

# Separate features and labels
X_train = train.drop(columns=['Activity'])
y_train = train['Activity']

X_test = test.drop(columns=['Activity'])
y_test = test['Activity']

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

"""**Random Forest**"""

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)


# Regularized Random Forest
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=4,
    random_state=42
)
model.fit(X_train, y_train_enc)

# Predict
y_pred = model.predict(X_test)

# Evaluation
print(" Accuracy:", accuracy_score(y_test_enc, y_pred))
print("\n Classification Report:\n")
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test_enc, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()

"""XGBOOST"""

# Encode labels
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)

# Define and train XGBoost model
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

model.fit(X_train, y_train_enc)

# Predict
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test_enc, y_pred))
print("\n Classification Report:\n")
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test_enc, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - XGBoost")
plt.tight_layout()
plt.show()

"""**GA**"""

# --- GA Setup ---
N_FEATURES = X_train.shape[1]

creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_bool", random.randint, 0, 1)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=N_FEATURES)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

def eval_individual(individual):
    selected = [i for i, bit in enumerate(individual) if bit == 1]
    if len(selected) == 0:
        return 0.0,
    clf = XGBClassifier(eval_metric='mlogloss', random_state=42)
    score = cross_val_score(clf, X_train.iloc[:, selected], y_train_enc, cv=3).mean()
    return score,

toolbox.register("evaluate", eval_individual)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)
toolbox.register("select", tools.selTournament, tournsize=3)

# --- Run GA ---
random.seed(42)
population = toolbox.population(n=20)
N_GEN = 10

best_individuals = []
for gen in range(N_GEN):
    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.2)
    fits = toolbox.map(toolbox.evaluate, offspring)
    for fit, ind in zip(fits, offspring):
        ind.fitness.values = fit
    population = toolbox.select(offspring, k=len(population))
    top = tools.selBest(population, 1)[0]
    best_individuals.append((gen, top.fitness.values[0]))
    print(f"Generation {gen}: Best Fitness = {top.fitness.values[0]:.4f}")

# --- Selected Features ---
best = tools.selBest(population, 1)[0]
selected_features = [i for i, bit in enumerate(best) if bit == 1]
selected_feature_names = X_train.columns[selected_features]
print("\nSelected Features:")
print(selected_feature_names.tolist())

# --- Plot fitness progress ---
gen_nums, fitness_vals = zip(*best_individuals)
plt.plot(gen_nums, fitness_vals, marker='o')
plt.title("GA Feature Selection Progress")
plt.xlabel("Generation")
plt.ylabel("Best Fitness (CV Accuracy)")
plt.grid(True)
plt.show()

"""**Selected Feature from GA with XGBoost Classifier (XGBClassifier)**


"""

# Train model on selected features
model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
model.fit(X_train[selected_feature_names], y_train_enc)

# Evaluate
y_pred = model.predict(X_test[selected_feature_names])

print(" Accuracy:", accuracy_score(y_test_enc, y_pred))
print("\n Classification Report:\n")
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test_enc, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - GA + XGBoost")
plt.tight_layout()
plt.show()

"""**Selected Features from GA in Random Forest**"""

# Train model on selected features using Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train[selected_feature_names], y_train_enc)

# Evaluate
y_pred_rf = rf_model.predict(X_test[selected_feature_names])

print("Accuracy:", accuracy_score(y_test_enc, y_pred_rf))
print("\nClassification Report:\n")
print(classification_report(y_test_enc, y_pred_rf, target_names=le.classes_))

# Confusion Matrix
cm_rf = confusion_matrix(y_test_enc, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - GA + Random Forest")
plt.tight_layout()
plt.show()

"""**PSO**"""

from pyswarm import pso
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import numpy as np

# Convert DataFrame to NumPy arrays
X_full = X_train.values
y_full = y_train_enc

# Use only training data for PSO (split train further into training + validation)
X_pso_train, X_pso_val, y_pso_train, y_pso_val = train_test_split(X_full, y_full, test_size=0.3, random_state=42)

# Define number of features
num_features = X_full.shape[1]
feature_names = X_train.columns

# Fitness function: returns 1 - accuracy (because PSO minimizes the objective)
def fitness_function(position):
    # Round to binary
    binary_position = np.round(position).astype(int)

    # If all zeros, return poor score
    if np.count_nonzero(binary_position) == 0:
        return 1.0

    # Select features
    selected_indices = np.where(binary_position == 1)[0]
    X_sel_train = X_pso_train[:, selected_indices]
    X_sel_val = X_pso_val[:, selected_indices]

    # Train and evaluate
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_sel_train, y_pso_train)
    y_pred = clf.predict(X_sel_val)

    acc = accuracy_score(y_pso_val, y_pred)
    return 1 - acc  # Because PSO minimizes

# Bounds: Each feature either 0 or 1 (binary mask)
lb = [0] * num_features
ub = [1] * num_features

# Run PSO
best_position, best_score = pso(fitness_function, lb, ub, swarmsize=30, maxiter=30)

# Convert best position to binary feature mask
best_features_mask = np.round(best_position).astype(int)
selected_feature_indices = np.where(best_features_mask == 1)[0]
selected_feature_names = feature_names[selected_feature_indices]

print("Selected features:", list(selected_feature_names))
print("Validation Accuracy with selected features:", 1 - best_score)

"""**Random Forest on PSO Selected Features**"""

# Train final RF model on selected features
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train[selected_feature_names], y_train_enc)

# Predict and evaluate on test set
y_pred_rf = rf_model.predict(X_test[selected_feature_names])

print("Accuracy:", accuracy_score(y_test_enc, y_pred_rf))
print("\nClassification Report:\n")
print(classification_report(y_test_enc, y_pred_rf, target_names=le.classes_))

# Confusion Matrix
cm_rf = confusion_matrix(y_test_enc, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - PSO + Random Forest")
plt.tight_layout()
plt.show()

"""**XGBOOST classifier on PSO selected**"""

# Train XGBoost on full training set with selected features
final_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
final_model.fit(X_train[selected_feature_names], y_train_enc)

# Predict and evaluate
y_pred = final_model.predict(X_test[selected_feature_names])

print("\n Final Test Accuracy:", accuracy_score(y_test_enc, y_pred))
print("\n Classification Report:\n", classification_report(y_test_enc, y_pred, target_names=le.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test_enc, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - PSO + XGBoost")
plt.tight_layout()
plt.show()